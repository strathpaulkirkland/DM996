{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNQh3RQO/Q8GS4+8S2ZzPrZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/strathpaulkirkland/DM996/blob/master/DM996_ANN_Example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Intro to Deep Learning Lab**\n",
        "\n",
        "In this lab we are going to basically fork an exisiting DL basics introduction and find out the issues with doing so. Enabling you to troubleshoot these issue later in the class yourself. \n",
        "Along with that by the end of this lab you should be able to build a NN to solve classification tasks.\n"
      ],
      "metadata": {
        "id": "PM4pABlrZnbS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Mount your google drive if you want to save any of the outputs from the lab\n",
        "## otherwise they are only stored temporarily. However you can click and download \n",
        "## items also. Otherwise just comment this out.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lI1ZMuT2YVd1",
        "outputId": "f72539bc-5c0e-4862-e0f8-c40d86cfde87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test to see if the instance is up and running correctly, checking both the CPU instance and GPU instance."
      ],
      "metadata": {
        "id": "Esd2nzqJpDdQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat /proc/cpuinfo # gives details of CPU\n",
        "\n",
        "!nvidia-smi # gives details of GPU\n"
      ],
      "metadata": {
        "id": "WWbQty2XpCzi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ex6NqM4lKaRg"
      },
      "source": [
        "## Part 0: Prerequisites:\n",
        "\n",
        "We recommend that you run this this notebook in the cloud on Google Colab (see link with icon at the top) if you're not already doing so. It's the simplest way to get started. You can also [install TensorFlow locally](https://www.tensorflow.org/install/). But, again, simple is best (with caveats):\n",
        "\n",
        "[tf.keras](https://www.tensorflow.org/guide/keras) is the simplest way to build and train neural network models in TensorFlow. So, that's what we'll stick with in this tutorial, unless the models neccessitate a lower-level API.\n",
        "\n",
        "Note that there's [tf.keras](https://www.tensorflow.org/guide/keras) (comes with TensorFlow) and there's [Keras](https://keras.io/) (standalone). You should be using [tf.keras](https://www.tensorflow.org/guide/keras) because (1) it comes with TensorFlow so you don't need to install anything extra and (2) it comes with powerful TensorFlow-specific features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3FgyF_4wKaRg"
      },
      "outputs": [],
      "source": [
        "# !pip uninstall tensorflow  #just incase you want to change version\n",
        "# !pip install tensorflow==2.X.0\n",
        "# TensorFlow and tf.keras\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Flatten, Dense\n",
        "\n",
        "# Commonly used modules\n",
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# Images, plots, display, and visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import IPython\n",
        "\n",
        "\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3z9JZ-KKaRi"
      },
      "source": [
        "## Part 1: Generic Simple ANN:\n",
        "### Boston Housing Price Prediction with Feed Forward Neural Networks\n",
        "\n",
        "Let's start with using a fully-connected neural network to do predict housing prices. The following image highlights the difference between regression and classification (see part 2). Given an observation as input, **regression** outputs a continuous value (e.g., exact temperature) and classificaiton outputs a class/category that the observation belongs to.\n",
        "\n",
        "<img src=\"https://i.imgur.com/vvSoAzg.jpg\" alt=\"classification_regression\" width=\"400\"/>\n",
        "\n",
        "For the Boston housing dataset, we get 506 rows of data, with 13 features in each. Our task is to build a regression model that takes these 13 features as input and output a single value prediction of the \"median value of owner-occupied homes (in $1000).\"\n",
        "\n",
        "Now, we load the dataset. Loading the dataset returns four NumPy arrays:\n",
        "\n",
        "* The `train_images` and `train_labels` arrays are the *training set*—the data the model uses to learn.\n",
        "* The model is tested against the *test set*, the `test_images`, and `test_labels` arrays."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QTX7MBKxKaRj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3afc4d7-4c4b-4580-817c-e1a9843c6a96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/boston_housing.npz\n",
            "57026/57026 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "(train_features, train_labels), (test_features, test_labels) = keras.datasets.boston_housing.load_data()\n",
        "\n",
        "# get per-feature statistics (mean, standard deviation) from the training set to normalize by\n",
        "train_mean = np.mean(train_features, axis=0)\n",
        "train_std = np.std(train_features, axis=0)\n",
        "train_features = (train_features - train_mean) / train_std"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3QQZBvx2KaRk"
      },
      "source": [
        "### Build the model\n",
        "\n",
        "Building the neural network requires configuring the layers of the model, then compiling the model. First we stack a few layers together using `keras.Sequential`. Next we configure the loss function, optimizer, and metrics to monitor. These are added during the model's compile step:\n",
        "\n",
        "* *Loss function* - measures how accurate the model is during training, we want to minimize this with the optimizer.\n",
        "* *Optimizer* - how the model is updated based on the data it sees and its loss function.\n",
        "* *Metrics* - used to monitor the training and testing steps.\n",
        "\n",
        "Let's build a network with 1 hidden layer of 20 neurons, and use mean squared error (MSE) as the loss function (most common one for regression problems):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N8fPA0CMKaRk"
      },
      "outputs": [],
      "source": [
        "def build_model():\n",
        "    model = keras.Sequential([\n",
        "        Dense(20, activation=tf.nn.relu, input_shape=[len(train_features[0])]),\n",
        "        Dense(1)\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer=tf.train.AdamOptimizer(), \n",
        "                  loss='mse',\n",
        "                  metrics=['mae', 'mse'])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "M0xsaN2oebsb"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3FGS5rHEKaRl"
      },
      "source": [
        "### Train the model\n",
        "\n",
        "Training the neural network model requires the following steps:\n",
        "\n",
        "1. Feed the training data to the model—in this example, the `train_features` and `train_labels` arrays.\n",
        "2. The model learns to associate features and labels.\n",
        "3. We ask the model to make predictions about a test set—in this example, the `test_features` array. We verify that the predictions match the labels from the `test_labels` array. \n",
        "\n",
        "To start training,  call the `model.fit` method—the model is \"fit\" to the training data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-HnDDXJWKaRl"
      },
      "outputs": [],
      "source": [
        "# this helps makes our output less verbose but still shows progress\n",
        "class PrintDot(keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs):\n",
        "        if epoch % 100 == 0: print('')\n",
        "        print('.', end='')\n",
        "\n",
        "model = build_model()\n",
        "\n",
        "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=50)\n",
        "history = model.fit(train_features, train_labels, epochs=1000, verbose=0, validation_split = 0.1,\n",
        "                    callbacks=[early_stop, PrintDot()])\n",
        "\n",
        "hist = pd.DataFrame(history.history)\n",
        "hist['epoch'] = history.epoch\n",
        "\n",
        "# show RMSE measure to compare to Kaggle leaderboard on https://www.kaggle.com/c/boston-housing/leaderboard\n",
        "rmse_final = np.sqrt(float(hist['val_mean_squared_error'].tail(1)))\n",
        "print()\n",
        "print('Final Root Mean Square Error on validation set: {}'.format(round(rmse_final, 3)))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Oh no, we have run into our first problem...\n",
        "\n",
        "AttributeError: module 'tensorflow._api.v2.train' has no attribute 'AdamOptimizer'\n",
        "\n",
        "go to [Tensorflow-Keras](https://www.tensorflow.org/api_docs/python/tf) to see what the problem is with tf.train...\n",
        "\n",
        "oh yeah it doesn't exist anymore (clearly this example was older)\n",
        "It has since been update to be within tf.keras.optimizers\n",
        "\n",
        "**Replace the tf.train.AdamOptimizer() in the build_model() function with an approriate Adam one found on the tensor flow site**\n",
        "\n",
        "Rerun this model then the training script again, we will find another issue.\n",
        "\n",
        "# Error 2\n",
        "\n",
        "KeyError: 'val_mean_squared_error'\n",
        "What does that mean\n",
        "\n",
        "It is suggesting that the line, seen below, has an error\n",
        "\n",
        "```\n",
        "rmse_final = np.sqrt(float(hist['val_mean_squared_error'].tail(1)))\n",
        "```\n",
        "\n",
        "it is looking for that value inside hist that is called val_mean_squared_error\n",
        "\n",
        "call the hist variable to see a print out of this variable\n"
      ],
      "metadata": {
        "id": "Kkk5GORFbC25"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hist\n"
      ],
      "metadata": {
        "id": "R-b3qCJB3EEC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Not that there is no val_mean_squared_error only a val mse.\n",
        "**Correct the above code and rerun.**"
      ],
      "metadata": {
        "id": "qEXT8McSb6lM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rmse_final = np.sqrt(float(hist['val_mse'].tail(1)))\n",
        "print()\n",
        "print('Final Root Mean Square Error on validation set: {}'.format(round(rmse_final, 3)))"
      ],
      "metadata": {
        "id": "w4nsHz8YcpOT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jr1LFNk6KaRl"
      },
      "source": [
        "Now, let's plot the loss function measure on the training and validation sets. The validation set is used to prevent overfitting ([learn more about it here](https://www.tensorflow.org/tutorials/keras/overfit_and_underfit)). However, because our network is small, the training convergence without noticeably overfitting the data as the plot shows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "v3qHyM56KaRm"
      },
      "outputs": [],
      "source": [
        "def plot_history():\n",
        "    plt.figure()\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Mean Square Error [Thousand Dollars$^2$]')\n",
        "    plt.plot(hist['epoch'], hist['mean_squared_error'], label='Train Error')\n",
        "    plt.plot(hist['epoch'], hist['val_mean_squared_error'], label = 'Val Error')\n",
        "    plt.legend()\n",
        "    plt.ylim([0,50])\n",
        "\n",
        "plot_history()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again we have an issue with the code and the naming of the mse terms replace the two above with mse"
      ],
      "metadata": {
        "id": "cdXVPgXYdXpB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_history():\n",
        "    plt.figure()\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Mean Square Error [Thousand Dollars$^2$]')\n",
        "    plt.plot(hist['epoch'], hist['mse'], label='Train Error')\n",
        "    plt.plot(hist['epoch'], hist['val_mse'], label = 'Val Error')\n",
        "    plt.legend()\n",
        "    plt.ylim([0,50])\n",
        "\n",
        "plot_history()"
      ],
      "metadata": {
        "id": "YtYg0xjVeIf1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWvvYqx_KaRm"
      },
      "source": [
        "Next, compare how the model performs on the test dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DWQAhffIKaRm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "756d90e6-2ea4-4b48-c535-0f3c05a5aa8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4/4 [==============================] - 0s 4ms/step - loss: 16.5975 - mae: 2.7258 - mse: 16.5975\n",
            "Root Mean Square Error on test set: 4.074\n"
          ]
        }
      ],
      "source": [
        "test_features_norm = (test_features - train_mean) / train_std\n",
        "mse, _, _ = model.evaluate(test_features_norm, test_labels)\n",
        "rmse = np.sqrt(mse)\n",
        "print('Root Mean Square Error on test set: {}'.format(round(rmse, 3)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hX0FoBzNKaRn"
      },
      "source": [
        "Compare the RMSE measure you get to the [Kaggle leaderboard](https://www.kaggle.com/c/boston-housing/leaderboard). An RMSE of 4 puts us in 20th place."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Improving the results\n",
        "Change the number of neurons from 20 to a higher and lower value and note the changes.\n",
        "\n",
        "Similairly change the optimizer to see if you can improve your result.\n",
        "\n",
        "See if you can find how to add learning rate modifications to the model, if not this will be covered in the next lab.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "tcP1Bb2ghyz2"
      }
    }
  ]
}